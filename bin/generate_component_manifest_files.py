# Copyright (c) 2024 Airbyte, Inc., all rights reserved.

import re
import sys
import tempfile
from glob import glob
from pathlib import Path

import anyio
import dagger
import httpx

PYTHON_IMAGE = "python:3.10"
LOCAL_YAML_DIR_PATH = "airbyte_cdk/sources/declarative"
LOCAL_OUTPUT_DIR_PATH = "airbyte_cdk/sources/declarative/models"

METADATA_SCHEMAS_GITHUB_URL = "https://api.github.com/repos/airbytehq/airbyte/contents/airbyte-ci/connectors/metadata_service/lib/metadata_service/models/src"
METADATA_SCHEMAS_RAW_URL_BASE = "https://raw.githubusercontent.com/airbytehq/airbyte/master/airbyte-ci/connectors/metadata_service/lib/metadata_service/models/src"
LOCAL_METADATA_OUTPUT_DIR_PATH = "airbyte_cdk/metadata_models/generated"

PIP_DEPENDENCIES = [
    "datamodel_code_generator==0.26.3",
]


def get_all_yaml_files_without_ext() -> list[str]:
    return [Path(f).stem for f in glob(f"{LOCAL_YAML_DIR_PATH}/*.yaml")]


def get_all_yaml_files_from_dir(directory: str) -> list[str]:
    return [Path(f).stem for f in glob(f"{directory}/*.yaml")]


def generate_init_module_content(yaml_files: list[str]) -> str:
    header = "# generated by bin/generate_component_manifest_files.py\n"
    for module_name in yaml_files:
        header += f"from .{module_name} import *\n"
    return header


async def download_metadata_schemas(temp_dir: Path) -> list[str]:
    """Download metadata schema YAML files from GitHub to a temporary directory."""
    headers = {
        "User-Agent": "airbyte-python-cdk-build",
        "Accept": "application/vnd.github.v3+json",
    }
    
    async with httpx.AsyncClient(headers=headers, timeout=30.0) as client:
        try:
            response = await client.get(METADATA_SCHEMAS_GITHUB_URL)
            response.raise_for_status()
            files_info = response.json()
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 403:
                print(
                    "Warning: GitHub API rate limit exceeded. Using cached schemas if available.",
                    file=sys.stderr,
                )
                raise
            raise
        
        yaml_files = []
        for file_info in files_info:
            if file_info["name"].endswith(".yaml"):
                file_name = file_info["name"]
                file_url = f"{METADATA_SCHEMAS_RAW_URL_BASE}/{file_name}"
                
                print(f"Downloading {file_name}...", file=sys.stderr)
                file_response = await client.get(file_url)
                file_response.raise_for_status()
                
                file_path = temp_dir / file_name
                file_path.write_text(file_response.text)
                yaml_files.append(Path(file_name).stem)
        
        return yaml_files


def replace_base_model_for_classes_with_deprecated_fields(post_processed_content: str) -> str:
    """
    Replace the base model for classes with deprecated fields.
    This function looks for classes that inherit from `BaseModel` and have fields marked as deprecated.
    It replaces the base model with `BaseModelWithDeprecations` for those classes.
    """

    # Find classes with deprecated fields
    classes_with_deprecated_fields = set()
    class_matches = re.finditer(r"class (\w+)\(BaseModel\):", post_processed_content)

    for class_match in class_matches:
        class_name = class_match.group(1)
        class_start = class_match.start()
        # Find the next class definition or end of file
        next_class_match = re.search(
            r"class \w+\(",
            post_processed_content[class_start + len(class_match.group(0)) :],
        )
        class_end = (
            len(post_processed_content)
            if next_class_match is None
            else class_start + len(class_match.group(0)) + next_class_match.start()
        )
        class_content = post_processed_content[class_start:class_end]

        # Check if any field has deprecated=True
        if re.search(r"deprecated\s*=\s*True", class_content):
            classes_with_deprecated_fields.add(class_name)

    # update the imports to include the new base model with deprecation warinings
    # only if there are classes with the fields marked as deprecated.
    if len(classes_with_deprecated_fields) > 0:
        # Find where to insert the base model - after imports but before class definitions
        imports_end = post_processed_content.find(
            "\n\n",
            post_processed_content.find("from pydantic.v1 import"),
        )
        if imports_end > 0:
            post_processed_content = (
                post_processed_content[:imports_end]
                + "\n\n"
                + "from airbyte_cdk.sources.declarative.models.base_model_with_deprecations import (\n"
                + "    BaseModelWithDeprecations,\n"
                + ")"
                + post_processed_content[imports_end:]
            )

    # Use the `BaseModelWithDeprecations` base model for the classes with deprecated fields
    for class_name in classes_with_deprecated_fields:
        pattern = rf"class {class_name}\(BaseModel\):"
        replacement = f"class {class_name}(BaseModelWithDeprecations):"
        post_processed_content = re.sub(pattern, replacement, post_processed_content)

    return post_processed_content


async def post_process_codegen(codegen_container: dagger.Container):
    codegen_container = codegen_container.with_exec(
        ["mkdir", "/generated_post_processed"], use_entrypoint=True
    )
    for generated_file in await codegen_container.directory("/generated").entries():
        if generated_file.endswith(".py"):
            original_content = await codegen_container.file(
                f"/generated/{generated_file}"
            ).contents()
            # the space before _parameters is intentional to avoid replacing things like `request_parameters:` with `requestparameters:`
            post_processed_content = original_content.replace(
                " _parameters:", " parameters:"
            ).replace("from pydantic", "from pydantic.v1")

            post_processed_content = replace_base_model_for_classes_with_deprecated_fields(
                post_processed_content
            )

            codegen_container = codegen_container.with_new_file(
                f"/generated_post_processed/{generated_file}", contents=post_processed_content
            )
    return codegen_container


async def post_process_metadata_models(codegen_container: dagger.Container):
    """Post-process metadata models to use pydantic.v1 compatibility layer."""
    codegen_container = codegen_container.with_exec(
        ["mkdir", "/generated_post_processed"], use_entrypoint=True
    )
    for generated_file in await codegen_container.directory("/generated").entries():
        if generated_file.endswith(".py"):
            original_content = await codegen_container.file(
                f"/generated/{generated_file}"
            ).contents()
            
            post_processed_content = original_content.replace(
                "from pydantic", "from pydantic.v1"
            )
            
            codegen_container = codegen_container.with_new_file(
                f"/generated_post_processed/{generated_file}", contents=post_processed_content
            )
    return codegen_container


async def generate_models_from_schemas(
    dagger_client: dagger.Client,
    yaml_dir_path: str,
    output_dir_path: str,
    yaml_files: list[str],
    post_process: bool = False,
    metadata_models: bool = False,
) -> None:
    """Generate Pydantic models from YAML schemas using datamodel-codegen."""
    init_module_content = generate_init_module_content(yaml_files)
    
    codegen_container = (
        dagger_client.container()
        .from_(PYTHON_IMAGE)
        .with_exec(["mkdir", "/generated"], use_entrypoint=True)
        .with_exec(["pip", "install", " ".join(PIP_DEPENDENCIES)], use_entrypoint=True)
        .with_mounted_directory(
            "/yaml", dagger_client.host().directory(yaml_dir_path, include=["*.yaml"])
        )
        .with_new_file("/generated/__init__.py", contents=init_module_content)
    )
    
    for yaml_file in yaml_files:
        codegen_container = codegen_container.with_exec(
            [
                "datamodel-codegen",
                "--input",
                f"/yaml/{yaml_file}.yaml",
                "--output",
                f"/generated/{yaml_file}.py",
                "--disable-timestamp",
                "--enum-field-as-literal",
                "one",
                "--set-default-enum-member",
                "--use-double-quotes",
                "--remove-special-field-name-prefix",
                "--field-extra-keys",
                "deprecated",
                "deprecation_message",
            ],
            use_entrypoint=True,
        )
    
    if post_process:
        codegen_container = await post_process_codegen(codegen_container)
        await codegen_container.directory("/generated_post_processed").export(output_dir_path)
    elif metadata_models:
        codegen_container = await post_process_metadata_models(codegen_container)
        await codegen_container.directory("/generated_post_processed").export(output_dir_path)
    else:
        await codegen_container.directory("/generated").export(output_dir_path)


async def main():
    async with dagger.Connection(dagger.Config(log_output=sys.stderr)) as dagger_client:
        print("Generating declarative component models...", file=sys.stderr)
        declarative_yaml_files = get_all_yaml_files_without_ext()
        await generate_models_from_schemas(
            dagger_client=dagger_client,
            yaml_dir_path=LOCAL_YAML_DIR_PATH,
            output_dir_path=LOCAL_OUTPUT_DIR_PATH,
            yaml_files=declarative_yaml_files,
            post_process=True,
        )
        
        print("\nGenerating metadata models...", file=sys.stderr)
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            metadata_yaml_files = await download_metadata_schemas(temp_path)
            
            await generate_models_from_schemas(
                dagger_client=dagger_client,
                yaml_dir_path=str(temp_path),
                output_dir_path=LOCAL_METADATA_OUTPUT_DIR_PATH,
                yaml_files=metadata_yaml_files,
                metadata_models=True,
            )
        
        print("\nModel generation complete!", file=sys.stderr)


anyio.run(main)
