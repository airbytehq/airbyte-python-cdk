# Copyright (c) 2024 Airbyte, Inc., all rights reserved.

import json
import re
import subprocess
import sys
import tempfile
from glob import glob
from pathlib import Path

import anyio
import dagger
import yaml

PYTHON_IMAGE = "python:3.10"
LOCAL_YAML_DIR_PATH = "airbyte_cdk/sources/declarative"
LOCAL_OUTPUT_DIR_PATH = "airbyte_cdk/sources/declarative/models"
LOCAL_METADATA_OUTPUT_DIR_PATH = "airbyte_cdk/test/models/connector_metadata/generated"

PIP_DEPENDENCIES = [
    "datamodel_code_generator==0.26.3",
]


def get_all_yaml_files_without_ext() -> list[str]:
    return [Path(f).stem for f in glob(f"{LOCAL_YAML_DIR_PATH}/*.yaml")]


def get_all_yaml_files_from_dir(directory: str) -> list[str]:
    return [Path(f).stem for f in glob(f"{directory}/*.yaml")]


def generate_init_module_content(yaml_files: list[str]) -> str:
    header = "# generated by bin/generate_component_manifest_files.py\n"
    for module_name in yaml_files:
        header += f"from .{module_name} import *\n"
    return header


def clone_metadata_schemas(temp_dir: Path) -> Path:
    """Clone metadata schema YAML files from GitHub using sparse checkout."""
    repo_url = "https://github.com/airbytehq/airbyte.git"
    schema_path = "airbyte-ci/connectors/metadata_service/lib/metadata_service/models/src"

    clone_dir = temp_dir / "airbyte"

    print("Cloning metadata schemas from airbyte repo...", file=sys.stderr)

    subprocess.run(
        [
            "git",
            "clone",
            "--depth",
            "1",
            "--filter=blob:none",
            "--sparse",
            repo_url,
            str(clone_dir),
        ],
        check=True,
        capture_output=True,
    )

    subprocess.run(
        ["git", "-C", str(clone_dir), "sparse-checkout", "set", schema_path],
        check=True,
        capture_output=True,
    )

    schemas_dir = clone_dir / schema_path
    print(f"Cloned schemas to {schemas_dir}", file=sys.stderr)

    return schemas_dir


def replace_base_model_for_classes_with_deprecated_fields(post_processed_content: str) -> str:
    """
    Replace the base model for classes with deprecated fields.
    This function looks for classes that inherit from `BaseModel` and have fields marked as deprecated.
    It replaces the base model with `BaseModelWithDeprecations` for those classes.
    """

    # Find classes with deprecated fields
    classes_with_deprecated_fields = set()
    class_matches = re.finditer(r"class (\w+)\(BaseModel\):", post_processed_content)

    for class_match in class_matches:
        class_name = class_match.group(1)
        class_start = class_match.start()
        # Find the next class definition or end of file
        next_class_match = re.search(
            r"class \w+\(",
            post_processed_content[class_start + len(class_match.group(0)) :],
        )
        class_end = (
            len(post_processed_content)
            if next_class_match is None
            else class_start + len(class_match.group(0)) + next_class_match.start()
        )
        class_content = post_processed_content[class_start:class_end]

        # Check if any field has deprecated=True
        if re.search(r"deprecated\s*=\s*True", class_content):
            classes_with_deprecated_fields.add(class_name)

    # update the imports to include the new base model with deprecation warinings
    # only if there are classes with the fields marked as deprecated.
    if len(classes_with_deprecated_fields) > 0:
        # Find where to insert the base model - after imports but before class definitions
        imports_end = post_processed_content.find(
            "\n\n",
            post_processed_content.find("from pydantic.v1 import"),
        )
        if imports_end > 0:
            post_processed_content = (
                post_processed_content[:imports_end]
                + "\n\n"
                + "from airbyte_cdk.sources.declarative.models.base_model_with_deprecations import (\n"
                + "    BaseModelWithDeprecations,\n"
                + ")"
                + post_processed_content[imports_end:]
            )

    # Use the `BaseModelWithDeprecations` base model for the classes with deprecated fields
    for class_name in classes_with_deprecated_fields:
        pattern = rf"class {class_name}\(BaseModel\):"
        replacement = f"class {class_name}(BaseModelWithDeprecations):"
        post_processed_content = re.sub(pattern, replacement, post_processed_content)

    return post_processed_content


async def post_process_codegen(codegen_container: dagger.Container):
    codegen_container = codegen_container.with_exec(
        ["mkdir", "/generated_post_processed"], use_entrypoint=True
    )
    for generated_file in await codegen_container.directory("/generated").entries():
        if generated_file.endswith(".py"):
            original_content = await codegen_container.file(
                f"/generated/{generated_file}"
            ).contents()
            # the space before _parameters is intentional to avoid replacing things like `request_parameters:` with `requestparameters:`
            post_processed_content = original_content.replace(
                " _parameters:", " parameters:"
            ).replace("from pydantic", "from pydantic.v1")

            post_processed_content = replace_base_model_for_classes_with_deprecated_fields(
                post_processed_content
            )

            codegen_container = codegen_container.with_new_file(
                f"/generated_post_processed/{generated_file}", contents=post_processed_content
            )
    return codegen_container


async def post_process_metadata_models(codegen_container: dagger.Container):
    """Post-process metadata models to use pydantic.v1 compatibility layer."""
    codegen_container = codegen_container.with_exec(
        ["mkdir", "/generated_post_processed"], use_entrypoint=True
    )
    for generated_file in await codegen_container.directory("/generated").entries():
        if generated_file.endswith(".py"):
            original_content = await codegen_container.file(
                f"/generated/{generated_file}"
            ).contents()

            post_processed_content = original_content.replace("from pydantic", "from pydantic.v1")

            codegen_container = codegen_container.with_new_file(
                f"/generated_post_processed/{generated_file}", contents=post_processed_content
            )
    return codegen_container


async def generate_models_from_schemas(
    dagger_client: dagger.Client,
    yaml_dir_path: str,
    output_dir_path: str,
    yaml_files: list[str],
    post_process: bool = False,
    metadata_models: bool = False,
) -> None:
    """Generate Pydantic models from YAML schemas using datamodel-codegen."""
    init_module_content = generate_init_module_content(yaml_files)

    codegen_container = (
        dagger_client.container()
        .from_(PYTHON_IMAGE)
        .with_exec(["mkdir", "/generated"], use_entrypoint=True)
        .with_exec(["pip", "install", " ".join(PIP_DEPENDENCIES)], use_entrypoint=True)
        .with_mounted_directory(
            "/yaml", dagger_client.host().directory(yaml_dir_path, include=["*.yaml"])
        )
        .with_new_file("/generated/__init__.py", contents=init_module_content)
    )

    for yaml_file in yaml_files:
        codegen_container = codegen_container.with_exec(
            [
                "datamodel-codegen",
                "--input",
                f"/yaml/{yaml_file}.yaml",
                "--output",
                f"/generated/{yaml_file}.py",
                "--disable-timestamp",
                "--enum-field-as-literal",
                "one",
                "--set-default-enum-member",
                "--use-double-quotes",
                "--remove-special-field-name-prefix",
                "--field-extra-keys",
                "deprecated",
                "deprecation_message",
            ],
            use_entrypoint=True,
        )

    if post_process:
        codegen_container = await post_process_codegen(codegen_container)
        await codegen_container.directory("/generated_post_processed").export(output_dir_path)
    elif metadata_models:
        codegen_container = await post_process_metadata_models(codegen_container)
        await codegen_container.directory("/generated_post_processed").export(output_dir_path)
    else:
        await codegen_container.directory("/generated").export(output_dir_path)


def consolidate_yaml_schemas_to_json(yaml_dir_path: Path, output_json_path: str) -> None:
    """Consolidate all YAML schemas into a single JSON schema file."""
    schemas = {}

    for yaml_file in yaml_dir_path.glob("*.yaml"):
        schema_name = yaml_file.stem
        with yaml_file.open("r") as f:
            schema_content = yaml.safe_load(f)
            schemas[schema_name] = schema_content

    all_schema_names = set(schemas.keys())
    
    for schema_content in schemas.values():
        if isinstance(schema_content, dict) and "definitions" in schema_content:
            all_schema_names.update(schema_content["definitions"].keys())

    def fix_refs(obj, in_definition=False):
        """Recursively fix $ref and type references in schema objects."""
        if isinstance(obj, dict):
            new_obj = {}
            for key, value in obj.items():
                if key == "$id" and in_definition:
                    continue
                elif key == "$ref" and isinstance(value, str):
                    if value.endswith(".yaml"):
                        schema_name = value.replace(".yaml", "")
                        new_obj[key] = f"#/definitions/{schema_name}"
                    else:
                        new_obj[key] = value
                elif key == "type" and isinstance(value, str) and value in all_schema_names:
                    new_obj["$ref"] = f"#/definitions/{value}"
                elif key == "type" and value == "const":
                    pass
                else:
                    new_obj[key] = fix_refs(value, in_definition=in_definition)
            return new_obj
        elif isinstance(obj, list):
            return [fix_refs(item, in_definition=in_definition) for item in obj]
        else:
            return obj

    # Find the main schema (ConnectorMetadataDefinitionV0)
    main_schema = schemas.get("ConnectorMetadataDefinitionV0")

    if main_schema:
        # Create a consolidated schema with definitions
        consolidated = {
            "$schema": main_schema.get("$schema", "http://json-schema.org/draft-07/schema#"),
            "title": "Connector Metadata Schema",
            "description": "Consolidated JSON schema for Airbyte connector metadata validation",
            **main_schema,
            "definitions": {},
        }

        # Add all schemas (including their internal definitions) as top-level definitions
        for schema_name, schema_content in schemas.items():
            if schema_name != "ConnectorMetadataDefinitionV0":
                if isinstance(schema_content, dict) and "definitions" in schema_content:
                    for def_name, def_content in schema_content["definitions"].items():
                        consolidated["definitions"][def_name] = fix_refs(def_content, in_definition=True)
                    schema_without_defs = {k: v for k, v in schema_content.items() if k != "definitions"}
                    consolidated["definitions"][schema_name] = fix_refs(schema_without_defs, in_definition=True)
                else:
                    consolidated["definitions"][schema_name] = fix_refs(schema_content, in_definition=True)

        consolidated = fix_refs(consolidated, in_definition=False)

        Path(output_json_path).write_text(json.dumps(consolidated, indent=2))
        print(f"Generated consolidated JSON schema: {output_json_path}", file=sys.stderr)
    else:
        print(
            "Warning: ConnectorMetadataDefinitionV0 not found, generating simple consolidation",
            file=sys.stderr,
        )
        Path(output_json_path).write_text(json.dumps(schemas, indent=2))


async def generate_metadata_models_single_file(
    dagger_client: dagger.Client,
    yaml_dir_path: str,
    output_file_path: str,
) -> None:
    """Generate all metadata models into a single Python file."""
    codegen_container = (
        dagger_client.container()
        .from_(PYTHON_IMAGE)
        .with_exec(["mkdir", "-p", "/generated_temp"], use_entrypoint=True)
        .with_exec(["pip", "install", " ".join(PIP_DEPENDENCIES)], use_entrypoint=True)
        .with_mounted_directory(
            "/yaml", dagger_client.host().directory(yaml_dir_path, include=["*.yaml"])
        )
    )

    codegen_container = codegen_container.with_exec(
        [
            "datamodel-codegen",
            "--input",
            "/yaml",
            "--output",
            "/generated_temp",
            "--disable-timestamp",
            "--enum-field-as-literal",
            "one",
            "--set-default-enum-member",
            "--use-double-quotes",
            "--remove-special-field-name-prefix",
            "--field-extra-keys",
            "deprecated",
            "deprecation_message",
        ],
        use_entrypoint=True,
    )

    generated_files = await codegen_container.directory("/generated_temp").entries()

    future_imports = set()
    stdlib_imports = set()
    third_party_imports = set()
    classes_and_updates = []

    for file_name in sorted(generated_files):
        if file_name.endswith(".py") and file_name != "__init__.py":
            content = await codegen_container.file(f"/generated_temp/{file_name}").contents()

            lines = content.split("\n")
            in_imports = True
            in_relative_import_block = False
            class_content = []

            for line in lines:
                if in_imports:
                    if line.startswith("from __future__"):
                        future_imports.add(line)
                    elif (
                        line.startswith("from datetime")
                        or line.startswith("from enum")
                        or line.startswith("from typing")
                        or line.startswith("from uuid")
                    ):
                        stdlib_imports.add(line)
                    elif line.startswith("from pydantic") or line.startswith("import "):
                        third_party_imports.add(line)
                    elif line.startswith("from ."):
                        in_relative_import_block = True
                        if not line.rstrip().endswith(",") and not line.rstrip().endswith("("):
                            in_relative_import_block = False
                    elif in_relative_import_block:
                        if line.strip().endswith(")"):
                            in_relative_import_block = False
                    elif line.strip() and not line.startswith("#"):
                        in_imports = False
                        class_content.append(line)
                else:
                    class_content.append(line)

            if class_content:
                classes_and_updates.append("\n".join(class_content))

    import_sections = []
    if future_imports:
        import_sections.append("\n".join(sorted(future_imports)))
    if stdlib_imports:
        import_sections.append("\n".join(sorted(stdlib_imports)))
    if third_party_imports:
        import_sections.append("\n".join(sorted(third_party_imports)))

    final_content = "\n\n".join(import_sections) + "\n\n\n" + "\n\n\n".join(classes_and_updates)

    post_processed_content = final_content.replace("from pydantic", "from pydantic.v1")

    lines = post_processed_content.split("\n")
    filtered_lines = []
    in_relative_import = False

    for line in lines:
        if line.strip().startswith("from . import"):
            in_relative_import = True
            if not line.rstrip().endswith(",") and not line.rstrip().endswith("("):
                in_relative_import = False
            continue

        if in_relative_import:
            if line.strip().endswith(")"):
                in_relative_import = False
            continue

        filtered_lines.append(line)

    post_processed_content = "\n".join(filtered_lines)

    codegen_container = codegen_container.with_new_file(
        "/generated/models.py", contents=post_processed_content
    )

    await codegen_container.file("/generated/models.py").export(output_file_path)


async def main():
    async with dagger.Connection(dagger.Config(log_output=sys.stderr)) as dagger_client:
        print("Generating declarative component models...", file=sys.stderr)
        declarative_yaml_files = get_all_yaml_files_without_ext()
        await generate_models_from_schemas(
            dagger_client=dagger_client,
            yaml_dir_path=LOCAL_YAML_DIR_PATH,
            output_dir_path=LOCAL_OUTPUT_DIR_PATH,
            yaml_files=declarative_yaml_files,
            post_process=True,
        )

        print("\nGenerating metadata models...", file=sys.stderr)
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            schemas_dir = clone_metadata_schemas(temp_path)

            output_dir = Path(LOCAL_METADATA_OUTPUT_DIR_PATH)
            output_dir.mkdir(parents=True, exist_ok=True)

            print("Generating single Python file with all models...", file=sys.stderr)
            output_file = str(output_dir / "models.py")
            await generate_metadata_models_single_file(
                dagger_client=dagger_client,
                yaml_dir_path=str(schemas_dir),
                output_file_path=output_file,
            )

            print("Generating consolidated JSON schema...", file=sys.stderr)
            json_schema_file = str(output_dir / "metadata_schema.json")
            consolidate_yaml_schemas_to_json(schemas_dir, json_schema_file)

        print("\nModel generation complete!", file=sys.stderr)


anyio.run(main)
